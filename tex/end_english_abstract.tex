%# -*- coding: utf-8-unix -*-
\begin{bigabstract}
With the continuous development of computer vision technology, augmented reality is more and more widely used in entertainment, industry, medical and other fields. But the educational applications is still limited. The particularity of educational applications is that educators need to edit virtual scenes independently to meet different teaching purposes, so it is necessary to develop a system for educators to create personalized experiments for teaching. At the same time, the educational applications are very necessary, because the experimental conditions of the school cannot meet the teaching needs sometimes, and the augmented reality technology can provide users with virtual assistance information such as experimental guidance and simulation phenomena. In addition, current augmented reality applications often fail to provide users with a sense of touch, which limits the interaction.

We propose a solution to the problem of lacking tactile sensation and high user requirements in educational augmented reality applications. Educators can edit and create virtual experiments, and experience in augmented reality scenarios through our systems. While obtaining virtual assistance information, the user can also have real tactile feelings.

We first analyze the research status of object tracking and authoring systems in China and abroad. Currently, object tracking could be divided into 2 steps - object detection and object tracing. It could also be divided into two types depending on the image - RGB image and RGB-D image. As for object tracking based on RGB image, the more traditional solution is the pattern recognition based algorithm proposed by Hinterstoisser et al\cite{hinterstoisser2011gradient}. There are also methods based on random forests\cite{brachmann2016uncertainty}, but the real-time performance is poor. The object tracing method which could reach the real-time is PWP3D algorithm proposed by Prisacariu and Reid\cite{prisacariu2012pwp3d}. However, solutions based on RGB images have higher limitations for object tracking, and are less mature than those based on RGB-D images. The latter could achieve better object detection effects through depth information. In the aspect of object tracing, the object pose can be solved by iterative closest point algorithm (ICP) or symbol distance function (SDF). At present, there are some authoring systems designed for education, but exist some problems such as limited interaction and functions.

Based on the research condition, we designed an augmented reality system. It has two parts: client and server. The server is primarily responsible for object tracking and transmits the tracking results to the client. The client has two main functions. Users could first customize the virtual experiment using the system, and then participate in the virtual experiment in the augmented reality scene. In the augmented reality scene, the client communicates with the server to send commands and receive object poses. After that, the client could rebuild a virtual object with the pose information and render it on the screen together with more virtual information like other static objects or hints.

We use some mature technology or tools. In terms of object tracking, we use Ren's tool LibISR\cite{Ren_3DV_2014, star3d_iccv_2013} to achieve object tracking. It implements a set of tracking functions that can track multiple objects in real time by processing RGB-D images and using the symbol distance function. In addition, images were acquired from Kinect 2 using Libfreenect 2\cite{libfreenect2}. It is a good, cross-platform, open source driver for Kinect 2. Finally, in the image processing part, the concurrent calculation is also performed by the CUDA\cite{CUDARef} code to improve the operation efficiency. In the client part, it is developed based on the Unity engine\cite{Unity}, and we use Vuforia\cite{Vuforia} to realize the function of the QR code recognition to align the virtual scene in Unity with the real scene. Besides, we publish it on the mobile terminal using the Android SDK. Since the client and server are developed with different languages, we try to use Protocol Buffer to serialize the data being transmitted, and then use TCP/IP for network transmission.

We identified user needs and technical needs. In terms of technology, the project should meet certain augmented reality needs. Augmented reality applications require attributes such as field of view, pixel per degree, latency, and accommodation to meet the needs of the human eye and provide a better user experience. The user's needs are decided mainly based on the intended users of this system â€“ the educators. In terms of interaction, they do not have professional knowledge and should be provided with user-friendly interfaces and sufficient assistance information. The user need to add objects, edit objects including its content, edit the experimental environment, edit the experimental procedure, and so on. In addition, the system needs to be able to provide rich and accurate virtual information for students' exam-oriented education and personalized education. Finally, due to the cost and the limitations of the school's teaching conditions, it is also necessary to use the mobile phone and occupy as little space as possible.

After that, we designed the architecture of the system. The design is still divided into three parts: server, client, and network communication. The core of the server is the object tracking tool LibISR, which includes the four main parts including acquiring camera data, network communication, user interface, and object tracking calculation. The main parts are supported by some assistant engines. At the bottom level, some persistent objects are also implemented to store data. Some data structures are also implemented and used, including vectors, images, matrices, and so on. The client is primarily developed based on the Unity engine. The top layer of the client implements the game manager, which is responsible for managing scene switching and saving user data. The game manager manages multiple scenarios such as login, registration, home page, editing experiments, and augmented reality virtual experiments. The scene in which the experiment was edited is the authoring system of the project. It provides a variety of UI controllers, as well as user input operations such as mouse clicks and drags. These UIs implement logic through the underlying engine and their data are stored in persistent objects. We achieve the functions in the requirements with these UIs. The augmented reality scene consists of three main modules, including Vuforia, which is responsible for the alignment of Unity scene and real scene, PCB module which communicates with the server, and the game manager. It first sends instructions to the server at runtime, then restores the object tracking data from the server to the virtual scene, and finally uses Vuforia to overlay the virtual information on the image from the mobile phone camera. The network communication module is responsible for the communication between the client and the server using TCP/IP. The client transmits user instructions to the server, including starting tracking, etc., and the server transmits the result of the object tracking to the client, and the message formats are pre-set through the Protocol Buffer.

When implementing the system, the server part is mainly based on the original LibISR tool. Based on that, functions such as acquiring Kinect 2 data, fusing RGB images with depth images, and displaying intermediate results are added. The client part is completely developed from zero. The game manager is first implemented by asynchronously loading the scene and singleton mode. The system was then implemented through Unity's UI controllers and coding. The editing system supports functions such as adding objects, editing container content properties, editing hint text properties, editing experimental processes, and editing experimental environments. Interactive system implementation is more complicated. The network part needs to exchange data in real time between the client and the server, and avoids sticking packages. Therefore, there are infinite loops in both the server and the client, and they are continuously sending messages and waiting for reply, so that the client and the server can communicate synchronously. The augmented reality part is implemented by Vuforia. Although object tracking has been implemented, the Kinect camera and the Unity virtual scene have different coordinate systems and need to be calibrated. Since the relative relationship of the axes is not clear, only continuous attempts can achieve the coordinate mapping. In addition, the virtual scene in Unity also needs to coincide with the real scene. Therefore, Vuforia is used to identify the QR code for calibration. Since the relative position of the QR code and Kinect will change frequently, the calibration needs to be done manually. To overview the whole procedure, the client obtains server data, converts the rotation matrix to quaternion, restores the traced object in Unity, calibrates the Unity scene with the real scene, and adds virtual information to complete the augmented reality interaction.

Finally, a certain analysis of the results of the system was carried out. In terms of the efficiency of object tracking, LibISR itself has relatively high efficiency, but due to network transmission and Unity lock frame issues, the FPS is less in client, but it does not affect the use experience. In terms of authoring the system, the user could achieve the target operations after some learning. Augmented reality applications could have relatively good experience after calibration. Virtual models and information could be appended in the result. We also develop some sample experimental operations, such as preheating the test tubes with alcohol lamps, collecting gas by gas cylinders, and so on.

At the same time, there are also some shortcomings in this project. For example, object tracking only supports single object and single model to be tracked currently. The effect of object tracking is greatly affected by illumination. The authoring system lacks user guidance, and the interactive system needs manual calibration. In the future, while solving these issues, more diverse tracking tools and better algorithms could be used.

\end{bigabstract}
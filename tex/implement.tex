\chapter{系统实现}
\label{implement}

本章节将对于上述设计的实现过程进行叙述。本章节分为四部分。第一部本系统如何基于LibISR的物体追踪系统\cite{Ren_3DV_2014, star3d_iccv_2013}实现后端服务器。第二部分将介绍使用Unity引擎实现编著系统。第三部分将介绍在客户端和服务器之间进行通讯的实现。最后一部分将介绍如何将服务器的物体追踪数据应用到最终的增强现实场景中实现交互的。

\section{物体追踪系统的实现}
本系统基于开源物体追踪工具，LIbISR进行实现。在使用原有算法的基础上，进行了一定的修改。

\subsection{Libfreenect 2的适配}
本项目基于Kinect第二代进行开发，但是原工具是基于第一代开发的。两代摄像头存在着一定的差异，本系统在开发过程中对于源程序进行了一定的修改。

首先原系统使用openni为驱动获取Kinect数据。但是openni并不能有效获取Kinect 2的数据，并且已经停止更新和维护了。因此本系统使用了Libfreenect 2作为替代。

LibISR对于不同的硬件提供了统一的接口ImageSourceEngine。它是一个纯虚类，只需要添加新的实现，并且在初始化的时候进行修改就可以使用不同的驱动了。在实现时，保存了Private data类，将驱动的不变数据，例如device指针，listener指针，以及从相机获取的数据进行保存。首先根据文档要求进行初始化，遍历连接的USB接口获取设备，选择使用CUDA进行加速，初始化RGB、红外线发生器、深度摄像头三个设备，配置深度摄像头的范围。Libfreenct 2还自带了使用Kinect默认标定数据生成RGB-D图像的功能，经过验证也具有比较好的融合效果，但是本系统仍然使用了自己的标定数据，Libfreenect 2只提供了融合之后的图片，而缺乏中间数据。此外，Libfreenect在初始化之后没有被使用的话就会被自动关闭，但是LibISR需要在初始化图像源之后进行其他操作，因此这里创建了新的线程，在初始化之后就会进入死循环，保证驱动不会自动关闭。

初始化之后，程序每一帧会调用该函数获取图像。获取图像时会首先获得数据在CPU当中的指针，然后将获取到的图像进行逐一复制。进行逐一赋值而不是memory set的原因是，LibISR中图像都是基于数组封装的，RGB图像摄像头获得的数据是BGR因此需要逐个转换，而深度图像摄像头获得的是32位float，而程序中实际使用的是16位short int。

转换完成后，就可以将获取的图像应用到后面的计算了。

\subsection{RGB-D图像融合}
由于Kinect 1的彩色图像分辨率为640*480，深度图像分辨率为320*240，两者成比例，并且可以通过设置驱动直接获得相同分辨率的图片，因此源程序存在应用错误、逻辑基于两者分辨率相同的情况。但是，Kinect 2的彩色图像分辨率为1920*1080，深度图像分辨率为512*424，不成比例。因此需要在获得图像后进行融合。

	融合首先需要获得标定数据。本系统通过iai-kinect提供的calibration tool完成标定之后，计算出了单应性矩阵。在Low Level Engine中实现了融合。为了运算效率考虑，系统中将深度图映射到了彩色图，使用深度图的分辨率。融合使用了CUDA代码，首先将数据从CPU复制到GPU，并且分配所需要的线程块和线程数目。在本系统中，使用了8*8的线程块（block）结构，以及64*53的网格结构，从而实现将每个像素分配到一个线程。由于CUDA代码中每个线程运行的时候都可以获得自己的block ID和thread ID，再加上block的结构，就很方便的计算出对应的像素位置。得到像素的位置，就可以利用上述的单应性矩阵公式计算出该像素在彩色图中对应的位置了。
	
\begin{equation}
 X_{rgb} = threadID.x + blockID.x * blockDims.x
\end{equation}
\begin{equation}
 Y_{rgb} = threadID.y + blockID.y * blockDims.y
\end{equation}

这一部分函数输出了两张图，一张是融合之后的RGB-D图，其中a通道代表深度值。另一张是对其的彩色图，即将RGB-D图像的a通道设为1，是彩色图缩小之后可以与深度图对其的版本。

\subsection{追踪结果显示}

由于Kinect 1中两幅图成比例，因此UIEngine可以在基于深度图大小计算出追踪结果后将其按比例放大绘制在彩色图上。但是，当二者不成比例时，追踪结果就会出现扭曲。因此这里使用了对其之后的rgb图像作为代替。虽然这样做不会有实质的影响，但是由于对其之后的彩色图存在已经造成的噪声和重影，会对显示效果有一定的影响。

\section{客户端的实现}
本小节将详细介绍按照设计的架构，依次介绍游戏管理器，以及应用中各场景的实现。

\subsection{游戏管理器}
游戏管理器会在基准场景（base scene）中生成，而且这个场景不会被销毁。在每次需要切换场景的时候，游戏管理器会先将当前场景（如果有的话）进行写在，然后启动coroutine异步加载另一个场景，该场景的加载模式为additive，表示在当前昌吉的基础上进行加载。加载完成之后改coroutine会进行返回，返回之后会根据新的场景中GM Settings的情况修改场景光照。如果加载过程太长的话，在这一个过程中可以添加等待场景加载的动画。

\subsection{编著场景}
编著功能主要在Build Experiment场景中进行实现，本小节将会按照功能需求进行逐个介绍。

\subsubsection{实验场景编辑}
实验环境编辑需要调整光照颜色、强度，相机高度、角度，实验环境（室内或是室外）等。因此使用了滑块（Slider）和开关（Switch）等控件，并添加一个半透明panel用于区分。滑块封装了OnValueChange函数，会在滑块发生移动的时候触发。当滑块发生移动的时候，会调用控制Slider的引擎，UI\_Slider，它保存了场景中相机、光源的指针，会读取此时slider的值，并且修改对应物体的属性值。相似的，开关控件会在修改的时候调用控制函数，如果选择根据实验环境的选择修改对应的场景模型。

\subsubsection{物体编辑}
场景中的物体首先通过菜单进行添加。点击添加物体按钮后，会播放动画将选择物体类别的菜单显示出来，选择要添加的物体类别，就可以显示所有支持物体的图片，点击对应图片就可以添加物体了。支持物体以及他们的种类通过静态二维链表储存。

选择物体种类的菜单由UI\_List实现。UI\_List本身具备垂直布局（vertical layout），它会将子对象自动按照竖直排列。而每一个子对象对应一种物体类别，并且绑定了onclick函数。而选择物体的菜单由UI\_Bag实现，他也包含了一个网格布局（grid layout），会自动将子对象按照网格排列。他的子对象都是预制体（prefab），可以呈现图片，并且绑定了创建物体的onclick函数。

当物体种类被选择后，UI\_List会根据二位链表查找对应物体的图片，并创建显示该图片的预制体，添加到选择物体的菜单中。用户选择物体之后，会通过对应物体的名字，在资源文件夹中查找对应的预制体游戏对象，添加到场景中，完成物体创建。

\subsubsection{提示信息编辑}
\subsubsection{实验流程编辑}
\subsubsection{编辑已有实验}

\subsection{交互场景}
\subsection{其他场景}
\subsubsection{登陆注册场景}
\subsubsection{主页}
\subsection{应用发布}
\subsubsection{分辨率适配}
\subsubsection{发布平台}
\section{网络通信实现}
